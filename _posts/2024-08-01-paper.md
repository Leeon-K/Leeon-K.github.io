---
layout: post
title: [Paper reading]-FlashAttention V3
date: 2024-08-01 00:32:13
description: this is what included tabs in a post could look like
tags: Papers
categories: sample-posts
tabs: true
---

## First tabs

7/11 刚开源的FA3，FA 和 Mamba原作者Tri Dao 团队的作品。
核心贡献
FA在H100这种新架构上只有35%的max Flops利用率，1.5-2.0x的fp16（baseline：FA2）。利用Tensor Cores和TMA进行3个提速的点：
1. Warp-specialization overlap计算和数据搬运；
2. 交错（异步？）block-wise matmul和softmax；
3. 解耦FP8低精度的硬件支持。
FAv1 v2
![alt text](image.png)
之前FA的介绍：https://zhuanlan.zhihu.com/p/668888063
Hopper新特性
H100新特性 
WGMMA
GPC技术的一个MMA指令，跟之前架构下的MMA指令的最大区别在于，WGMMA支持warp-group的能力。之前的MMA指令都是基于单warp的，这项技术基于Hopper的Thread blocks cluster （Graphics Processing Clusters（GPC））技术和distributed shared memory。

![alt text](image-1.png)Vollta~Ampere架构，TensorCore指令就是WMMA令人头秃的cudaTensorCoreGemm详解_wmma cuda-CSDN博客；
TMA 异步执行（Tensor Memory Accelerator）

FP8

FA3


疑问：
[] FA3核心在于基于Hopper的新特性做实现上的优化，但是Hopper 22年就出了，之前这么长时间都没有基于H系列卡的优化吗？
Ref
https://zhuanlan.zhihu.com/p/708409249
https://tridao.me/blog/2024/flash3/  官方博客
https://tridao.me/publications/flash3/flash3.pdf paper

RingAttention

https://github.com/zhuzilin/ring-flash-attention